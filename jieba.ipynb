{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**三种常用分词方法**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''1.基于字典（基于字符串匹配）'''\n",
    "#比如最大正向匹配\n",
    "class MMSeg():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.segment_dict = {\"伟大\", \"中国\", \"勤劳\",\"中国人民\", \"北京\", \"人民政府\"}#分词词典，一般比较大\n",
    "        self.hot_dict = {\"伟大\", \"中国\", \"中国人民\", \"北京\", \"人民政府\"}#热词词表\n",
    "        self.max_word_length = 4#词语的最大长度，可调\n",
    "        \n",
    "    def segment(self, text):\n",
    "        index  = 0#存储当前索引\n",
    "        words = []#存储分词结果\n",
    "        while index<len(text):\n",
    "            #每一轮匹配，就是在一个窗口内，遍历所有前缀紫子字符串\n",
    "            #首先计算窗口最右端的索引。快到文本末尾时，需要截断。\n",
    "            window_end_index = min(len(text), index + self.max_word_length)\n",
    "            #传进来的是指针，如果在text末尾添加3个符号，会改变text;深拷贝一个的话，还需要操作内存，消耗也挺大。\n",
    "            word = None#用于存储本轮匹配到的词语\n",
    "            for i in range(window_end_index, index, -1):#由长到短匹配\n",
    "                sub_string = text[index:i]\n",
    "                if sub_string in self.segment_dict:\n",
    "                    word = sub_string\n",
    "                    break#如果匹配到合适的词语，更短的就不需要考虑了\n",
    "            if word==None:#如果没有匹配到词语，index对应的字单独成词，索引加一\n",
    "                words.append(text[index]) \n",
    "                index += 1\n",
    "            else:#如果匹配到词语，收集这个词语，索引加word的长度\n",
    "                words.append(word) \n",
    "                index += len(word) \n",
    "        return words \n",
    "    \n",
    "    def find_hot_words(self, text):\n",
    "        index  = 0#存储当前索引\n",
    "        hot_words = []#存储识别出的热词\n",
    "        while index<len(text):\n",
    "            #每一轮匹配，就是在一个窗口内，遍历所有前缀紫子字符串\n",
    "            #首先计算窗口最右端的索引。快到文本末尾时，需要截断。\n",
    "            window_end_index = min(len(text), index + self.max_word_length)\n",
    "            \n",
    "            hot_word = None#用于存储本轮匹配到的词语\n",
    "            for i in range(window_end_index, index, -1):#由长到短匹配\n",
    "                sub_string = text[index:i]\n",
    "                if sub_string in self.hot_dict:\n",
    "                    hot_word = sub_string\n",
    "                    break#如果匹配到合适的词语，更短的就不需要考虑了\n",
    "            if hot_word==None:#如果没有匹配到词语，索引加一\n",
    "                index += 1\n",
    "            else:#如果匹配到热词，收集这个词语，索引加word的长度\n",
    "                hot_words.append(hot_word) \n",
    "                index += len(hot_word) \n",
    "        return hot_words\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    seg = MMSeg()\n",
    "    s  =\"伟大的中国人民是勤劳的\"\n",
    "#     s = \"在中国人民政府是很重要的\"\n",
    "    print(seg.segment(s))#打印分词结果\n",
    "    print(seg.find_hot_words(s))#打印热词识别结果  \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "corpus=[\"我\",\"今天\",\"特别\",\"特别想\",\"非常\",\"非常想\",\"吃\",\"芒果\"]\n",
    "sentence=\"我今天特别想吃芒果啊\"\n",
    "max_num=3#最大词条长度，这里一眼就看出来，对于大的语料词典，用下边计算\n",
    "max_num=0\n",
    "for i in corpus:\n",
    "    if len(i)>max_num:\n",
    "        max_num=len(i)\n",
    "def word_segment(corpus,sentence,max_num):\n",
    "    start=0\n",
    "    result=[]\n",
    "    flag=0\n",
    "    while start<len(sentence):\n",
    "        window_end=min(len(sentence),start+max_num)#滑动窗口\n",
    "        word=None\n",
    "        for i in range(window_end,0,-1):\n",
    "            sub_string=sentence[start:window_end]\n",
    "            if sub_string in corpus:\n",
    "                word=sub_string\n",
    "                break\n",
    "        if word:\n",
    "            result.append(word)\n",
    "            start+=len(word)\n",
    "        else:\n",
    "            result.append(sentence[start])\n",
    "            start+=1\n",
    "    return result\n",
    "\n",
    "word_segment(corpus,sentence,max_num)              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''基于统计\n",
    "1.找出句子的所有分词结果\n",
    "2.在所有结果里找到最好的一个，用语言模型也可以'''\n",
    "from collections import defaultdict\n",
    "#针对步骤一：类似于LeetCode140，单词匹配II那个，返回所有匹配的结果\n",
    "def wordBreak(s,wordDict,dic):\n",
    "    if not s:\n",
    "        return []\n",
    "    if s in dic:\n",
    "        return dic[s]\n",
    "    res=[]\n",
    "    for word in wordDict:\n",
    "        n=len(word)\n",
    "        if s[:n]==word:\n",
    "            for r in wordBreak(s[n:],wordDict,dic):\n",
    "                if r:\n",
    "                    res.append(word+\" \"+r)\n",
    "\n",
    "    return res\n",
    "    \n",
    "corpus={\"我们\",\"我\",\"今天\",\"特别\",\"特别想\",\"非常\",\"非常想\",\"吃\",\"芒果\",\"想\"}\n",
    "sentence=\"我今天特别想吃芒果\"\n",
    "print(wordBreak(sentence,corpus,{}))\n",
    "#步骤二：找最好的分词结果，类似于这个\n",
    "'''文本分词，基于词典，计算最大分数的切分组合：\n",
    "https://www.cnblogs.com/naniJser/p/6058775.html\n",
    "类似于jieba的DAG ，\n",
    "从后往前，动态规划问题：动态转移方程：route[idx] = max((log(dict.FREQ.get(sentence[idx:x + 1]) or 1) -\n",
    "                              logtotal + route[x + 1][0], x) for x in DAG[idx])'''\n",
    "\n",
    "DAG={0: [0], 1: [1, 2, 4], 2: [2], 3: [3, 4], 4: [4]}\n",
    "s=\"去北京大学\"\n",
    "word_scores={\"北京大学\":100,\"北京大\":0,\"大学\":50,\"去\":100,\"玩\":100,\"北京\":20,\"北\":1,\"京\":1,\"大\":1,\"学\":1}\n",
    "def findMaxScorepath(s,DAG,word_scores):\n",
    "    n=len(s)\n",
    "    dp=[0]*(n+1)\n",
    "    #从后往前计算\n",
    "    for i in range(n-1,-1,-1):\n",
    "        #word_scores.get(s[i:x+1],0)表示如果该分词方式不在字典里，则为0\n",
    "        dp[i]=max([word_scores.get(s[i:x+1],0)+dp[x+1] for x in DAG[i]])\n",
    "    return dp[0]\n",
    "findMaxScorepath(s,DAG,word_scores)\n",
    "#[100+100=200]\n",
    "\n",
    "#HMM分词\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DAG**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''有向无环图构建：然后基于前缀词典，对输入文本进行切分，对于“去”，没有前缀，那么就只有一种划分方式；对于“北”，\n",
    "则有“北”、“北京”、“北京大学”三种划分方式；对于“京”，也只有一种划分方式；对于“大”，\n",
    "则有“大”、“大学”两种划分方式，依次类推，可以得到每个字开始的前缀词的划分方式。'''\n",
    "def get_DAG(sentence):\n",
    "    global FREQ\n",
    "    DAG={}\n",
    "    N=len(sentence)\n",
    "    for k in range(N):\n",
    "        tmplist=[]\n",
    "        i=k\n",
    "         # 位置k形成的片段\n",
    "        frag=sentence[k]\n",
    "        #如果该字或词在字典里边:\n",
    "        while i<N and frag in FREQ:\n",
    "            # 如果该片段的词频大于0\n",
    "            # 将该片段加入到有向无环图中\n",
    "            # 否则，继续循环\n",
    "            if FREQ[frag]:\n",
    "                tmplist.append(i)#自己的位置\n",
    "            i+=1\n",
    "            # 新的片段较旧的片段右边新增一个字\n",
    "            frag=sentence[k:i+1] \n",
    "#             print(frag)\n",
    "        #生成这个词的有向无环图\n",
    "        DAG[k] = tmplist\n",
    "    return DAG\n",
    "    #简单构建一个词和相应词频字典\n",
    "FREQ={\"北京大学\":2053,\"北京大\":0,\"大学\":20025,\"去\":12340,\"玩\":4207,\"北京\":34488,\"北\":17860,\"京\":6583,\"大\":144099,\"学\":17482}\n",
    "sentence=\"去北京大学\"\n",
    "DAG=get_DAG(sentence)\n",
    "DAG       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''动态规划：因为汉语句子的重心经常落在后面, 就是落在右边, 因为通常情况下形容词太多,\n",
    "后面的才是主干, 因此, 从右往左计算, 正确率要高于从左往右计算, 这个类似于逆向最大匹配！'''\n",
    "from math import log\n",
    "total=sum(FREQ.values(),0)\n",
    "def calc(sentence,DAG):  #动态规划，计算最大概率的切分组合\n",
    "\t#输入sentence是句子，DAG句子的有向无环图\n",
    "    N = len(sentence)  #句子长度\n",
    "    min_freq=1\n",
    "    logtotal=log(total)\n",
    "    route={}\n",
    "    route[N] = (0.0,'')\n",
    "    for idx in range(N-1,-1,-1):  #和range用法一样，不过还是建议使用xrange\n",
    "\t\t#可以看出是从后往前遍历每个分词方式的\n",
    "\t   #下面的FREQ保存的是每个词在dict中的频度得分，打分的公式是 log(float(v)/total)，其中v就是被打分词语的频数\n",
    "\t\t #FREQ.get(sentence[idx:x+1,min_freq)表示，如果字典get没有找到这个key，那么我们就使用最后的frequency来做\n",
    "\t\t #由于DAG中是以字典+list的结构存储的，所以确定了idx为key之外，\n",
    "\t\t #仍然需要for x in DAG[idx]来遍历所有的单词结合方式（因为存在不同的结合方法，例如“国”，“国家”等）\n",
    "\t\t #以（频度得分值，词语最后一个字的位置）这样的tuple保存在route中\n",
    "        candidates = [ (log(FREQ.get(sentence[idx:x+1]) or min_freq)-logtotal+ route[x+1][0],x) for x in DAG[idx] ]\n",
    "        route[idx] = max(candidates)\n",
    "    return route[0]\n",
    "calc(sentence,DAG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
